\chapter{Numerical Linear Algebra}

\section{Triangular systems and Gaussian elimination}

\begin{algorithm}[H]\caption{Backwards elimination}
        To solve the $n\times n$ linear system $Ax = b$ with 
        \begin{equation*}
        A = 
        \begin{bmatrix}
            a_{11} & a_{12} & \dots & a_{1n}\\
            0 & a_{22} & \dots & a_{1n}\\
            \vdots & & & \vdots\\
            0 & 0 & \dots & a_{nn}
        \end{bmatrix},
        \qquad
        b = \begin{bmatrix}
            b_1 \\
            b_2 \\
            \vdots\\
            b_n
        \end{bmatrix}.
    \end{equation*}   
        \\
        \KwIn{Number of unknowns and equations $n$;\newline 
        Matrix $A = [a_{ij}]$, with $1\leq i \leq n$ and $1\leq j \leq n$;\newline Vector $b = [b_i]$, with $1\leq i\leq n$;}
        \KwOut{Solution $x_1,x_2,\dots,x_n$ or message that the linear system has no unique solution.}
        \medskip
        $x_n = b_n/a_{nn}$\;
        \For{$i=n-1:1$}{
            \medskip
            $x_i = [b_{i} - \sum_{j=i+1}^n a_{ij}x_j]/a_{ii}$.
        }
    \end{algorithm}


\begin{algorithm}[H]\caption{Gaussian elimination}
        To solve the $n\times n$ linear system
        \begin{gather*}
            E_1:  a_{11}x_1 + a_{11}x_1 + \cdots + a_{1n}x_1 = a_{1,n+1}\\
            E_2:  a_{21}x_1 + a_{22}x_1 + \cdots + a_{2n}x_1 = a_{2,n+1}\\
            \vdots \qquad\qquad \vdots\qquad \qquad\vdots  \\
             E_n:  a_{n1}x_1 + a_{n2}x_1 + \cdots + a_{nn}x_1 = a_{n,n+1}
        \end{gather*}\\
        \KwIn{Number of unknowns and equations $n$;\newline 
        Augmented matrix $\tilde{A} = [a_{ij}]$, with $1\leq i \leq n$ and $1\leq j \leq n+1$.}
        \KwOut{Solution $x_1,x_2,\dots,x_n$ or message that the linear system has no unique solution.}
        \medskip
        \For{$i=1:n-1$}{
            Let $p$ be the smallest integer with $i\leq p\leq n$ and $a_{pi}\neq 0$\\
            \If{ 'No integer $p$ can be found' } {
                OUTPUT ('No unique solution exists')\;
                STOP\;
            }
            \If{$p\neq i$}{
                Perform $E_p \leftrightarrow E_i$
            }
            \For{j=i+1:n}{
                $m_{ij} = a_{ji}/a_{ii}$\;
                $E_j = E_j - m_{ji}E_i$\;
            }
        }
        Find $x_1,x_2,\dots,x_n$ Solving Triangular System with backwards substitution.
    \end{algorithm}




\hrule 
\begin{exercise} 
    Write the pseudocode and implement and algorithm to solve a Lower triangular system by adapting the Backwards Elimination Algortihm for Upper triangular Systems.
\end{exercise}

\begin{exercise}
    
\end{exercise}


\newpage

\section{LU Factorization}
    
    From what we have seen so far, solving a general linear system of the form 
        $$Ax = b$$
    requires $\mathcal{O}(n^3)$ operations. Instead, if the matrix is in upper triangular (resp lower triangular) form, solving the system only requires $\mathcal{O}(n^2)$ ops. This reduces considerably the amount of work that has to be done. \par
    If we want to solve several problems involving the same matrix $A$ it can be useful to transform it into a form that only requires a small amount of extra work for each new problem. That is when the $LU$ factorization comes in handy. \par
    Basically, the objective is to write (whenever it is possible) $A = LU$, with $L$ a lower triangular matrix and $U$ and upper triangular one. Thus, the linear system 
        $$LU x = A x = b$$
    can be solved in two easy steps.
    \begin{enumerate}
        \item Solve $Ly = b$
        \item Solve $Ux = y$.
    \end{enumerate}
    Then, the amount of work for each new $b$ is of order $\mathcal{O}(2n^2)$.\par
    To find $L$ and $U$ we simply have to adapt the Gaussian elimination procedure. Let us assume for the moment that we can perform Gaussian elimination without pivoting (pivoting = row interchanges). Remember that after each step, the matrix $A$ is modified. We will denote each successive modification by $A^{(i)}$. Starting with $A^{(1)} := A$, and finishing after $n-1$ steps with an upper triangular matrix  $U:= A^{(n)}$. \par
    The first step in Gaussian elimination consists on eliminating the entries below the diagonal in the first column of $A$ performing the operations, 
    \begin{equation*}
        (E_j) = E_j - m_{j1} E_1, \qquad\text{with } m_{j1} = \frac{a_{21}^{(1)}}{a_{11}^{(1)}}, \qquad j=2,\dots,n.
    \end{equation*}
    This can also be written using \defin{Gaussian transformation matrices $M^{(i)}$} of the form 
    \begin{align*}
        A^{(2)} = M^{(1)} A^{(1)},
        \intertext{with }
        M^{(1)} := 
        \begin{bmatrix}
            1 &0 &0 &\dots &0\\
            -m_{21} &1 &0 &\dots &0\\
            -m_{31} &0 &1 &\dots &0\\
            \vdots &\vdots &\vdots &\vdots &\vdots \\
            -m_{n1} &0 &0 &\dots &1
        \end{bmatrix}.
    \end{align*}
    Analogously, for the next steps, to transform $A^{(k)}$ into $A^{(k+1)}$ we perform
    \begin{equation*}
        (E_j) = E_j - m_{jk} E_k, \qquad\text{with } m_{jk} = \frac{a_{jk}^{(k)}}{a_{kk}^{(k)}}, \qquad j=k+1,\dots,n.
    \end{equation*}
    Which in matrix form is 
    \begin{align*}
        A^{(k+1)} = M^{(k)} A^{(k)},
        \intertext{with }
        M^{(k)} := 
        \begin{bmatrix}
            1 &0   &0 &0 &\dots &0\\
            0 &1   &0 &0 &\dots &0\\
            0 &0   &1 &0 &\dots &0\\
            \vdots &\vdots &\vdots &1 &\vdots &\vdots \\
            \vdots &\vdots &\vdots &-m_{k+1,k} &\vdots &\vdots \\
            \vdots &\vdots &\vdots &\vdots &\vdots &\vdots \\
            0 &0   &0 &-m_{n,k} &\dots &1
        \end{bmatrix}.
    \end{align*}
    With this notation,
    \begin{equation*}
        U = A^{(n)} = M^{(n-1)}M^{(n-2)}\cdots M^{(1)} A.    
    \end{equation*}
    Now, since all the $M^{(i)}$ are lower triangular, its product is lower triangular and we denote it $L^{-1} := M^{(n-1)}M^{(n-2)}\cdots M^{(1)}$. Moreover, since $L^{-1}$ is lower triangular, its inverse $L$ will be lower triangular as well. Therefore, 
    \begin{equation*}
        LU = L \cdot \left[M^{(n-1)}M^{(n-2)}\cdots M^{(1)}\right] A = L \cdot L^{-1} A = A.    
    \end{equation*}
    Let us try to find explicitly $L$. First, notice that $[M^{(i)}]^{-1}$ should be the matrix that reverts the operations 
    \begin{equation*}
        (E_j) = E_j - m_{jk} E_k, \qquad\text{with } m_{jk} = \frac{a_{jk}^{(k)}}{a_{kk}^{(k)}}, \qquad j=k+1,\dots,n.
    \end{equation*}
    Therefore, it will perform the operations 
    \begin{equation*}
        (E_j) = E_j + m_{jk} E_k, \qquad\text{with } m_{jk} = \frac{a_{jk}^{(k)}}{a_{kk}^{(k)}}, \qquad j=k+1,\dots,n.
    \end{equation*}
    Or, in matrix form 
    \begin{equation*}
        [M^{(k)}]^{-1}=
        \begin{bmatrix}
            1 &0   &0 &0 &\dots &0\\
            0 &1   &0 &0 &\dots &0\\
            0 &0   &1 &0 &\dots &0\\
            \vdots &\vdots &\vdots &1 &\vdots &\vdots \\
            \vdots &\vdots &\vdots &m_{k+1,k} &\vdots &\vdots \\
            \vdots &\vdots &\vdots &\vdots &\vdots &\vdots \\
            0 &0   &0 &m_{n,k} &\dots &1
        \end{bmatrix}.
    \end{equation*}
    Now since 
    \begin{equation*}
        L = [M^{(1)}]^{-1} \cdots [M^{(n-1)}]^{-1},
    \end{equation*}
    we can check that 
    \begin{equation*}
        L = 
        \begin{bmatrix}
            1 &0   &0 &0 &\dots &0\\
            m_{21} &1   &0 &0 &\dots &0\\
            m_{31} &m_{32}   &1 &0 &\dots &0\\
            \vdots &\vdots &\vdots &1 &\vdots &\vdots \\
            \vdots &\vdots &\vdots &m_{k+1,k} &\vdots &\vdots \\
            \vdots &\vdots &\vdots &\vdots &\vdots &\vdots \\
            m_{n1} &m_{n2}   &m_{n3} &m_{n,k} &\dots &1
        \end{bmatrix}.
    \end{equation*}
    
    
    Following this ideas we get the following algorithm. 
    \begin{algorithm}\caption{LU factorization}
        To factor the $n\times n$ matrix $A = [a_{ij}]$ into the product of the lower and upper triangular matrices $L =[l_{ij}]$ and $U=[u_{ij}]$. That is, $A = LU$. 
        
        \KwIn{Dimension $n$;\newline
        Matrix $A$;}
        \KwOut{Entries $l_{ij}$, $1\leq i \leq n$, $1<j\leq i$ of $L$. Stored in the lower triangular part of $A$;\newline
        Entries $u_{ij}$,  $1\leq i \leq n$, $i\leq j\leq n$ of $U$. Stored in the upper triangular part of $A$;}
        \medskip
        \For{i=1,\dots,n-1}{
            \If{$a_{ii} = 0$}{
                OUTPUT(`Factorization Impossible')\;
                STOP\;
            } 
            \For(\tcp*[f]{Modify rows $i+1$ to $n$}){$k=i+1,\dots,n$}{
                $m = a_{ki}/a_{ii}$ \tcp*{Multiplier between rows $k$ and $i$}
                \For(\tcp*[f]{Modify entries of row k}){$j=i+1:n$}{
                    $a_{kj} = a_{kj}-ma_{ij}$\;
                }  
                $a_{ki} = m$ \tcp*{Save multiplier in lower part of $A$}
            }
            
            
        }
        OUTPUT(A)\;
    \end{algorithm}
    
    
    
    Previous algorithm assumes that Gaussian elimination of $A$ can be performed without pivoting (row interchanges). In case that is not possible, there is still hope to find a good decomposition of a permutation of $A$ instead. By \defi{permutation of A} we mean a matrix that is found by just rearranging the rows of $A$. All we have to notice is that if two rows are interchanged in the middle of the process, that is equivalent to change them before starting the $LU$ decomposition and then proceeding without pivoting. Thus, the only thing that we need to add is a way to track where each row finished. We introduce the variables $p_i$ representing which initial row of $A$ ended in the position $i$. Before writing the algorithm, we will see a specially useful pivoting strategy that is done to avoid round-off error propagation.\par
    If the elements $a_{kk}^{(k)}$ are small in magnitude compared $a_{jk}^{(k)}$, then the magnitude of the multiplier 
    \begin{equation*}
        m_{jk} = \frac{a_{jk}^{(k)}}{a_{kk}^{(k)}}
    \end{equation*}
    will be much larger than 1. Round-off error introduced in the row $E_k$ will be then greatly amplified when we do $E_j = E_j - m_{jk}E_k$. Therefore, is is useful to interchange row $k$ with the row below $k$ that has the bigger element in column $k$. The  algorithm ends as follows, 
    
    \begin{algorithm}\caption{LU factorization with partial row pivoting}
        To factor the $n\times n$ permutation of  matrix $A = [a_{ij}]$ into the product of the lower and upper triangular matrices $L =[l_{ij}]$ and $U=[u_{ij}]$. That is, $PA = LU$. Where $P$ is a permutation matrix. 
        
        \KwIn{Dimension $n$;\newline
        Matrix $A$;}
        \KwOut{Entries $l_{ij}$, $1\leq i \leq n$, $1<j\leq i$ of $L$. Stored in the lower triangular part of $A$;\newline
        Entries $u_{ij}$,  $1\leq i \leq n$, $i\leq j\leq n$ of $U$. Stored in the upper triangular part of $A$;\newline
        Numbers $p_i$, $i=1,\dots,n$, representing what row of $A$ finished in the position $i$;}
        \medskip
        Set $p_i = i$ for $i=1,\dots,n$\;
        \For{i=1,\dots,n-1}{
            Find $l$ such that $|a_{li}| = \max_{i\leq j \leq n} |a_{ji}|$ \tcp*{Find row with biggest element at column $i$}
            $p_i \leftrightarrow p_l$\;
            $a_{i,j}\leftrightarrow a_{l,j}$, $j=1,\dots,n$\;
         
            \For(\tcp*[f]{Modify rows $i+1$ to $n$}){$k=i+1,\dots,n$}{
                $m = a_{ki}/a_{ii}$ \tcp*{Multiplier between rows $k$ and $i$}
                \For(\tcp*[f]{Modify entries of row k}){$j=i+1:n$}{
                    $a_{kj} = a_{kj}-ma_{ij}$\;
                }  
                $a_{ki} = m$ \tcp*{Save multiplier in lower part of $A$}
            }
        }
        OUTPUT(A, $p_i$, $i=1,\dots,n$)\;
    \end{algorithm}
    
    
    There are other types of pivoting strategies, but in general they require too much extra work for the added benefits. We will not cover those extra pivoting strategies in this course. \par
    
    One question that remains open is, under which conditions a matrix $A$ has an $LU$ decomposition. In general to find that out one just runs the algorithm and see if it can finish without errors. Nevertheless, there are some theoretical result that can also be used in some circumstances. 
    
    \begin{definition}
        Let $A = (a_{ij})$ be an $n\times n$ matrix. Its leading principal submatrices are the square matrices 
        \begin{equation*}
            A_k = 
            \begin{bmatrix}
                    a_{11} &\cdots &a_{1k}\\
                    \vdots & &\vdots\\
                    a_{k1} &\cdots &a_{kk}
            \end{bmatrix}, \qquad
            k = 1, \dots, n.
        \end{equation*}
    \end{definition}
    
    \begin{theorem}
        Let $A$ be an $n\times n$ matrix whose leading principal submatrices $A_1,\dots,A_n$ are all nonsingular. Then, there exists an $n\times n$ lower triangular matrix $L$, with ones on its diagonal, and an $n\times n$ upper triangular matrix $U$ such that $A=LU$ and this factorization is unique.
        
    \end{theorem}
    
    \begin{proof}
        Since the first leading principal $A_1$ is non-singular, then $a_{11}\neq 0$ and we do not need to exchange rows in the first step of Gaussian elimination. Suppose that this is also true for steps $2,\dots,k-1$ so that $A^{(k)} = M^{(k-1)} \cdots M^{(1)} A$. That is,
        
        \scalebox{0.8}{\begin{minipage}{1.2\textwidth}
        \begin{align*}
            &\begin{bmatrix}
                a_{11}^{(k)} &\cdots   &a_{1k}^{(k)} &\cdots &\cdots  &a_{1n}^{(k)}\\
                %\ddots &   &\vdots & & & &\vdots \\
                 &\ddots &\vdots & &  &\vdots\\
                &   &a_{kk}^{(k)}   &\cdots &\cdots &a_{kn}^{(k)}\\
                 & &\vdots & &  &\vdots  \\
                 \vdots & &\vdots & & & \vdots\\
                0 &\cdots &a_{nk}^{(k)} &\cdots &\cdots  &a_{nn}^{(k)} 
            \end{bmatrix} = \\
            &\begin{bmatrix}
                1 & & & & & &\\
                -m_{21} &\ddots & & & & & \\
                \vdots & &1 & & & &\\
                -m_{k+1,1} &\cdots    &-m_{k+1,k} &1   & & &\\
                \vdots & &\vdots & &\ddots & &\\
                %\vdots & &\vdots & &  &\ddots &\\
                -m_{n1} & & &\cdots & & &1
            \end{bmatrix}
            \begin{bmatrix}
                a_{11} &\cdots   &a_{1k}  &\cdots &\cdots &a_{1n}\\
                %\vdots &   &\vdots & & & &\vdots \\
                \vdots &   &\vdots & &  &\vdots \\
                a_{k1} &\cdots &a_{kk}  &\cdots &\cdots &a_{kn}\\
                \vdots & &\vdots & &  &\vdots\\
                \vdots & &\vdots & &  &\vdots\\
                a_{n1} &\cdots &a_{nk}  &\cdots &\cdots&a_{nn}  \\
            \end{bmatrix}.
        \end{align*}
        \end{minipage}}
        
        The determinant of the leading $k\times k$ leading principal submatrix $A_k$ on the left is $a_{11}a_{22}^{(2)}\cdots a_{kk}^{(k)}$ and this is equal to the determinant of the product of the $k\times k$ leading principal submatrices on the right hand. Since the determinant of the leading submatrix of $M^{(k-1)}\cdots M^{(1)}$ is one (It is a triangular matrix with ones in the diagonal), it follows that 
        \begin{equation*}
            a_{11}a_{22}^{(2)}\cdots a_{kk}^{(k)}= \det(A_k)\neq 0
        \end{equation*}
        which implies that $a_{kk}^{(k)}\neq 0$ and the $k$-th step of Gaussian elimination can be done without row interchanges. Following this inductive procedure we can conclude that $A = LU$.\par
        Let us show that the decomposition is unique. Suppose $A= L_1U_1 = L_2U_2$. Then, 
        \begin{equation*}
            L_2^{-1}L_1 = U_2U_1^{-1}.
        \end{equation*}
        But the matrix on the left hand side is lower triangular with ones in its diagonal whereas the one on the right hand side is upper triangular. Therefore, we must have
        \begin{equation*}
            L_2^{-1}L_1 = I = U_2U_1^{-1},
        \end{equation*}
        which implies $L_1=L_2$ and $U_1=U_1$.
    \end{proof}
    
    
    Two special families of matrices satisfy the hypotheses of this theorem. 
    
    \begin{definition}
        An $n \times n$ matrix $A$ is said \defi{diagonally dominant} when 
        \begin{equation*}
            |a_{ii}|\geq \sum_{j=1,j\neq i}|a_{ij}|, \qquad \text{for } i = 1,2,\dots,n.
        \end{equation*}
        A diagonally dominant matrix is said to be \defi{strictly diagonally dominant} when the inequality is strict. That is, when $\geq$ is replaced by $>$.
    \end{definition}
    
    \begin{definition}
        A matrix $A$ is \defi{positive definite} if it is symmetric and if $x^T A x>0$ for every vector $x$. Note: Not everyone requires symmetry for a positive definite matrix. Always check the definition given because this one changes from author to author.
    \end{definition}
  
    Therefore, we can conclude 
    
    \begin{corollary}
        Let $A$ be an $n \times n$ matrix. Then $A= LU$, where $L$ is an $n\times n$ lower triangular matrix, with ones on its diagonal, and $U$ is an $n \times n$ upper triangular matrix if either
        \begin{enumerate}
            \item $A$ is strictly diagonally dominant.
            \item A is symmetric positive definite.
        \end{enumerate}
    \end{corollary}
    
    When a matrix has a special structure, like being positive definite or sparse (too many zeros), the number of operations in matrix factorization can be cut down exploiting this features. For example, if $A$ is symmetric we could decompose $A=LL^T$, where $L$ is a triangular matrix with positive entries in its diagonal. This representation is called Cholesky factorization. Note that the matrix $L$ is not the same that we obtained with $LU$ factorization. Nevertheless, the method to find this $L$ can also be adapted to $LU$ factorization.
    
    \begin{theorem}
        Let $A$ be a symmetric positive definite matrix. Then, there is a unique lower triangular matrix $L$ with positive entries in its diagonal such that $A=LL^T$.
    \end{theorem}
    
    \begin{proof}
        Instead of the proof, we just give the method to find the entries of the matrix $L$ below.
    \end{proof}
    
    \begin{method}(Cholesky factorization)
        Cholesky factorization is obtained by writting $A=LL^T$ and exploiting the lower triangular structure of $L$ and the symmetry of $A$ as follows. First, $L =[\ell_{ij}]$ is lower triangular, then 
        \begin{equation*}
            a_{ij}=\sum_{k=1}^n \ell_{ik}\ell_{jk} = \sum_{k=1}^{\min\{i,j\}}\ell_{ik}\ell_{jk}.
        \end{equation*}
        Now, because $A^T=A$ we only need $a_{ij}$ for $i\leq j$, that is
        \begin{equation*}
            a_{ij}= \sum_{k=1}^i\ell_{ik}\ell_{jk}\qquad 1\leq i \leq j \leq n.
        \end{equation*}
        We can solve this system of equations for the entries of $L$ one column at a time. For the first column we set
        \begin{align*}
            &a_{11}= \ell_{11}^2 &\rightarrow \ell_{11}=\sqrt{a_{11}}\\
            &a_{12} = \ell_{11}\ell_{21} &\rightarrow \ell_{21}= a_{11}/ \ell_{11}\\
            &\ \vdots\\
            &a_{1n}=\ell_{11}\ell_{n1} &\rightarrow \ell_{n1} =a_{1n}/\ell_{11}.
        \end{align*}
        The second column is now found by 
        \begin{align*}
            &a_{22}= \ell_{21}^2 + \ell_{22}^2 &\rightarrow \ell_{22}=\sqrt{a_{22}-\ell_{21}^2}\\
            &a_{23} = \ell_{21}\ell_{31} + \ell_{22}\ell_{32} &\rightarrow \ell_{32}= (a_{23}-\ell_{21}\ell_{31}) / \ell_{22}\\
            &\ \vdots\\
            &a_{2n}=\ell_{21}\ell_{n1} + \ell_{22}\ell_{n2} &\rightarrow \ell_{n2} =(a_{2n}-\ell_{21}\ell_{n1}) /\ell_{nn}.
        \end{align*}
        Continuing with this process we get all the columns of $L$.
    \end{method}
    
    \begin{algorithm}[h!]\caption{Cholesky factorization} 
        To factor the positive definite matrix $A$ into $LL^T$, where $L$ is lower triangular\\
        \KwIn{Matrix $A$;}
        \KwOut{Entries $\ell_{ij}$, for $1\leq i \leq j \leq n$ of $L$;}
        \For{i=1:n}{
            $\ell_{ii} = \sqrt{a_{ii}-\sum_{k=1}^{i-1}\ell_{ik}^2}$\;
            \For{j=i+1:n}{
                $\ell_{ji} = (a_{ij} - \sum_{k=1}^{i-1}\ell_{ik}\ell_{jk}) / \ell_{ii}$\;
            }
        }
        
        
        
    \end{algorithm}
  
    A similar type of procedure can be followed to find a fast $LU$ factorization of tridiagonal systems. 
    
    \begin{theorem}
        If $A$ is tridiagonal and all of its leading principal submatrices are non-singular then
        \begin{align}\label{tri_LU}
            \begin{bmatrix}
                a_1 &b_1 & & & \\
                c_1 &a_2 &b_2 & &\\
                 &\ddots &\ddots &\ddots & \\
                 & &c_{n-2} &a_{n-1} &b_{n-2}\\
                  & & &c_{n-1} &a_n
            \end{bmatrix} =
            \begin{bmatrix}
                1 & & & & \\
                \ell_1 &a_2 & & &\\
                 &\ddots &\ddots & & \\
                 & &\ell_{n-2} &a_{n-1} &\\
                  & & &\ell_{n-1} &a_n
            \end{bmatrix} 
            \begin{bmatrix}
                m_1 &b_1 & & & \\
                 &m_2 &b_2 & &\\
                 & &\ddots &\ddots & \\
                 & & &m_{n-1} &b_{n-2}\\
                  & & & &m_n
            \end{bmatrix}
        \end{align}
        where 
        \begin{align*}
            &m_1 = a_1\\
            &\ell_j = c_j/m_j\\
            &m_{j+1} = a_{j+1} - \ell_j b_j \qquad j=1,\dots,n-1.
        \end{align*}
        And this factorization is unique. Moreover, it can be computed in $\mathcal{O}(n)$ operations and the corresponding linear system can be solved efficiently.
    \end{theorem}
    
    \begin{proof}
        We know that $A$ has a unique $LU$ factorization, where $L$ is unit lower triangular and $U$ is upper triangular. We will show that we can solve uniquely for $\ell_1,\dots,\ell_{n-1}$ and $m_1,\dots,m_n$ so that the given factorization holds. Equating the matrix product on the right hand side of \eqref{tri_LU} row by row we have
        \begin{align*}
            &a_1 = m_1,& \ & &b_1 = b_1\\
            &c_1 = m_1\ell_1,&\ &a_2 = \ell_1 b_1 + m_2,\ &b_2 = b_2\\
            &\vdots\\
            &c_{n-1} = m_{n-1}\ell_{n-1},& \ &a_n = \ell_{n-1}b_{n-1}+m_n &
        \end{align*}
        form which the given equations follow. We need to prove that the $m_j$'s are nonzero to solve for $\ell_j$. First note that,
        \begin{equation*}
            m_{j+1} = a_{j+1}-\ell_jb_j = a_{j+1}-\frac{c_j}{m_j}b_j.
        \end{equation*}
        So we have 
        \begin{equation*}
            m_jm_{j+1} = a_{j+1}m_j - c_jb_j.
        \end{equation*}
        Now, 
        \begin{align*}
            &\det(A_1) = a_1 = m_1,\\
            &\det(A_2) = a_2a_1 -c_1b_1 = a_2_m_1-c_1b_1 = m_1 m_2.
        \end{align*}
        We now do induction to show that $\det(A_k)=m_1m_2\cdots m_k$. Suppose $\det(A_j) =m_1m_2\cdots m_j$ for $j=1,\dots,k-1$. Expanding by the last row and using the inductive hypothesis we get
        \begin{align*}
            \det(A_k)&=a_k \det(A_{k-1}) - b_{k-1}c_{k-1}\det(A_{k-2})\\
            &= m_1m_2\cdots m_{k-2}(a_km_{k-1}-b_{k-1}c_{k-1}) \\
            &=m_1m_2\cdots m_{k},
        \end{align*}
        for $k=1,\dots,n$. Since, by hypothesis, $\det(A_k)\neq 0$ for every $k$ then $m_1,\dots, m_n$ are all nonzero. 
    \end{proof}
    
    \begin{algorithm}[h!]\caption{Tridiagonal solver for $Ax=d$} 
        To solve the system $Ax=b$ for a tridiagonal matrix $A$;\\
        \KwIn{Matrix $A$;\newline
        Vector $d$;}
        \KwOut{Solution $x$;}
        $m_1 = a_1$\;
        \For(\tcp*[f]{Compute $LU$ decomposition}){$j=1:n-1$}{
            $\ell_j = c_j/m_j$\;
            $m_{j+1}= a_{j+1}-\ell_jb_j$\;
        }
        \For(\tcp*[f]{Forward substitution $LY = d$}){$j=2:n$}{
            $y_j = d_j - \ell_{j-1}y_{j-1}$\;
        }
        $x_n = y_n/m_n$\;
        \For(\tcp*[f]{Backwards substitution $Ux = y$}){$j=n-1:-1:1$}{
            $x_j=y_j-b_jx_{j+1}$\;
        }
    \end{algorithm}
  
  \bigskip
  \bigskip
  \hrule

    \begin{exercise}
        Solve the following systems using $LU$ decomposition with pivoting. Verify your answer.
        \begin{enumerate}
            \item 
                \begin{align*}
                    4x_1-x_2+x_3 = -1,\\
                    -x_1+3x_2 = 4,\\
                    x_1+2x_3 = 5.
                \end{align*} 
                \item 
                \begin{align*}
                    1x_1+2x_2-x_3 = 0,\\
                    2x_1+4x_2+0x_3 = 1,\\
                    0x_1+1x_2-x_3 = 5.
                \end{align*} 
                \item 
                \begin{align*}
                    0x_1+1x_2+1x_3+2x_4 = 0,\\
                    0x_1+1x_2+1x_3-x_4 = 1,\\
                    1x_1+2x_2-x_3 +3x_4= 2,\\
                    1x_1+1x_2+2x_3 +x_4= 3,\\
                \end{align*} 
        \end{enumerate}
    \end{exercise}
    
    \begin{exercise}
        Write an algorithm to find the inverse of a matrix $A$ by solving the system $AX=I$. That is, $X=A^{-1}I$. Count the number of operations required.
        Verify your algorithm with some examples.
    \end{exercise}  
    
    \begin{exercise}
        Count the number of operations needed in Cholesky factorization.
    \end{exercise}
      
    
    \begin{exercise}
        Count the number of operations needed to solve a tridiagonal linear system using the Tridiagonal solver algorithm.
    \end{exercise}  
  
  
\newpage  
\section{Norms of Vectors and Matrices. Condition number. Error analysis} 

    To discuss the errors in numerical problems involving vectors, it is useful to employ \defi{norms}. Our vectors are usually in $\R^n$, but a norm can be defined on any vector space. 
    
    \begin{definition}
        A \defi{vector norm} on $\R^n$ is a function, $||\cdot||:\R^n \rightarrow \R$ with the following properties,
        \begin{enumerate}
            \item $||x||\geq0$ for all $x\in \R^n$
            \item $||x||=0$ if and only if $x=0$
            \item $||\alpha x||= |\alpha|\  ||x||$ for all $\alpha\in \R^n$ and $x\in \R^n$
            \item $||x+y||\leq ||x||+||y||$ for all $x,y\in \R^n$.
        \end{enumerate}
    \end{definition}
    
    We can think of $||x||$ as the length or magnitude of a vector $x$. A norm on a vector generalizes the notion of absolute value, $|r|$, for a real or complex number. The most common norms in $\R^n$ are the so called $\ell_p$ norms defined by
    \begin{align*}
        &||x||_1 = \sum_{i=1}^n |x_i|\\
        &||x||_p = \left( \sum_{i=1}^n |x_i|^p\right)^{1/p}, \qquad \text{for } 1<p<\infty\\
        &||x||_\infty = \max_{1\leq i \leq n}|x_i|.
    \end{align*}
    With the \defi{Euclidean $\ell_2$ norm} being the most used one.
    
    To understand these norms better, it is instructive to consider $\R^2$. For different $\ell_p$ norms we sketch the set 
    \begin{equation*}
        \{x\in \R^2: ||x||\leq 1\}
    \end{equation*}
    \begin{figure}[h!]
        \centering
        \includegraphics[scale=0.4]{Unit balls.png}
    \end{figure}
    This set is called the \defi{unit ball} in $\R^2$.
    
    \begin{definition}
        The norm of a vector gives a measure for the distance between an arbitrary vector and the zero vector, just as the absolute value of a real (or complex) number describes the distance from $0$. Similarly, the \defi{distance between two vectors} is defined as the norm of their difference. That is,
        \begin{equation*}
            d(x,y) := ||x-y||_p.
        \end{equation*}
        Notice that the distance changes if we use different $\ell_p$ norms.
    \end{definition}

    If $d(x,y)$ is small we say that the vectors are close. In particular, we can use norms to define limits as we did with the euclidean norm. 
    
    \begin{definition}
        A sequence $\{x^{(k)}\}_{k=1}^\infty$ of vectors in $\R^n$ is said to \defi{converge to x with respect to the norm $||\cdot ||$} if, given $\varepsilon>0$, there exists an integer $N(\varepsilon)$ such that
        \begin{equation*}
            ||x^{(k)} - x||<\varepsilon, \qquad \text{for all } k\geq N(\varepsilon).
        \end{equation*}
    \end{definition}
    
    Different norms will modify the way the sequences converge. Nevertheless, in finite dimensional spaces (as $\R^n$) all norms are equivalent with respect to convergence. That is,
    
    \begin{lemma}
        Let $\{x^{(k)}\}_{k=1}^\infty$ a sequence in $\R^n$ that converges to $x$ in some norm $|| \cdot ||$. Then, it converges to $x$ in any norm.
    \end{lemma}%Proofrequired
    
    One useful way of understanding what convergence in finite dimensional spaces mean is to notice that 
    
    \begin{theorem}
        A sequence $\{x^{(k)}\}_{k=1}^\infty$ converges to $x$ in $\R^n$ with respect to the $\ell_\infty$ norm if and only if $\lim_{k\rightarrow \infty} x^{(k)}_i = x_i$  \defi{uniformly in $i$} for each $i=1,2,\dots,n$. That is, given $\varepsilon>0$, there exists an integer $N(\varepsilon)$ such that
        \begin{equation*}
            |x^{(k)}_i -x_i|<\varepsilon, \qquad \text{for } i=1,\dots,n \text{ and } k\geq N(\varepsilon).
        \end{equation*}
        This is why the $\ell_\infty$ norm is called the uniform convergence norm. Each coordinate of the sequence gets closer to each component of the limit at the exact same rate.
    \end{theorem} %Proofrequired
    
    Notice that since all norms are equivalent respect to convergence in finite dimensional spaces then this result is valid for every norm. This is not the case when working in infinite dimensional spaces like function spaces!

    
    We now turn to the question of defining norms for matrices. Although we could deal with general matrix norms, asking for the same four properties than any other norm, we usually prefer a matrix norm that is related to a vector norm.
    
    
    \begin{definition}
        If $||\cdot ||$ is a vector norm in $\R^n$, then the \defi{natural, or induced, matrix norm} associated to the vector norm is defined by 
        \begin{equation*}
            ||A|| = \max_{||x||=1}||Ax||.
        \end{equation*}
    \end{definition}
    
    \begin{lemma} 
        Induced matrix norms are norms. That is, they satisfy the four properties of norms. Even more, they also satisfy three additional properties,
        \begin{itemize}
            \item $||Ax|| \leq ||A||\ ||x||$
            \item $|| I || = 1$
            \item $||AB|| \leq ||A||\ ||B||$.
        \end{itemize}
    \end{lemma}
    
    \begin{definition} 
        The distance between matrices is defined in the same way as for vectors. Given a matrix norm $||\cdot ||$, the distance between two matrices $A$ and $B$ is given by $d(A,B) = ||A-B||$.
    \end{definition}
    
    Finding matrix norms just by the definition might be difficult sometimes. Some close formulas can be found for special norms.
    
    \begin{theorem}
        Let $A = [a_{ij}]$ be an $n\times n$ matrix, then 
        \begin{align*}
            ||A||_\infty = \max_{1\leq i \leq n} \sum_{j=1}^n |a_{ij}|\\
            ||A||_1 = \max_{1\leq j \leq n} \sum_{i=1}^n |a_{ij}|\\
            ||A||_2 = \sqrt{\rho(A^TA)},
        \end{align*}
        where $$\rho(A^TA)$$ is called \defi{the spectral radius of $A^TA$} and is defined as the largest eigenvalue of $A^TA$.
    \end{theorem}%Proofrequired
    
    
    
    Now we introduce an important quantity in numerical linear algebra that is useful to identify how errors propagate when solving linear systems $Ax = b$. 
    
    \begin{definition}
        The \defi{condition number} of the non-singular matrix $A$ relative to the norm $||\cdot||$ is
            $$\kappa(A) = ||A||\ ||A^{-1}||.$$
    \end{definition}
    
    It turns out that it is not necessary to compute the inverse of $A$ to obtain an estimate of the condition number.\par
    If the linear system is sensitive to perturbations in the elements of $A$ or the components of $b$, then this fact is reflected in $A$ having a large condition number. In such case, the matrix $A$ is said to be \defi{ill-conditioned}. Briefly, the larger the condition number, the more ill-conditioned the system. The following two results make this statement precise. 
    
    \begin{theorem}
        Let $A\in \R^{n\timesn}$ a non-singular matrix. Let $b$, $\delta b \in \R^n$ with
            $$Ax=b, \qquad A(x+\delta x) = b + \delta b.$$
        Then, 
        \begin{equation*}
            \frac{1}{\kappa(A)}\frac{||\delta b||}{||b||}\leq \frac{||\delta x||}{||x||} \leq \kappa(A) \frac{||\delta b||}{||b||}.
        \end{equation*}
        Moreover, for some $b_1$, $\delta b_1$ (resp. $b_2$, $\delta b_2$) the lower (resp.  upper) equality is attained.
    \end{theorem}
    
    \begin{proof}
        We have that $A(\delta x) = \delta b$. Therefore,
        \begin{equation*}
            \frac{||\delta x||}{||x||} \leq
            \frac{||A^{-1} \delta b||}{||x||}\leq
            \frac{||A^{-1}||\ || \delta b||}{||x||} \frac{||b||}{||b||} =\frac{||A^{-1}||\ || \delta b||}{||x||} \frac{||Ax||}{||b||}\leq \frac{||A^{-1}||\ ||A||\ || \delta b||}{||b||}.
        \end{equation*}
        For the other inequality apply the same reasoning to the system $A^{-1}b= x$.
    \end{proof}
    
    Basically, this says that the relative error in the solution of the system is bounded by $\kappa(A)$ times the relative error in the data $b$.
    
    \begin{theorem}
        Let $A\in \R^{n\times n}$ a non-singular matrix and $\delta A$ a perturbation matrix such that
         $$||\delta A||<\frac{1}{||A^{-1}||}.$$
        Let $b$, $\delta b \in \R^n$ and $\tilde{x}=x+\delta x$ satisfying 
            $$(A+\delta A)\tilde{x}=b+\delta b, \qquad\text{and } Ax = b.$$
        Then, 
        \begin{equation*}
            \frac{||\delta x||}{||x||} = \frac{||x-\tilde{x}||}{||x||} \leq \frac{\kappa(A)||A||}{||A||-\kappa(A)||\delta A||}\left(\frac{||\delta b||}{||b||} + \frac{||\delta A||}{||A||}\right).
        \end{equation*}
    \end{theorem} %Proofrequired
    
    
    An interesting application of the condition number is that it gives a relative distance to the closest singular matrix to $A$.
    
    \begin{theorem}
        Let $A\in \R^{n\times n}$ be a non-singular matrix and $||\cdot ||$ any induced matrix norm. Then,
        \begin{equation*}
            \frac{1}{\kappa(A)} = \inf_{B \ singular} \frac{||A-B||}{||A||}.
        \end{equation*}
    \end{theorem}
    
    In fact, to determine if a matrix is singular, a big condition number is a better indicative than a small determinant!
    
    
\bigskip
\hrule
\bigskip

\begin{exercise}
    Find the $\ell_1$, $\ell_2$ and $\ell_\infty$ norms of the vectors
    \begin{enumerate}
        \item x = [3, -4, 0, 3/2]
        \item x = [2, 5, -1, 33, 6, 7, 22]
    \end{enumerate}
\end{exercise}

\begin{exercise}
    Find the $\ell_1$ and $\ell_\infty$ norms of the matrices
    \begin{enumerate}
        \item $A = [10,15 ; 0,1]$
        \item $B = [1/3, -1/3, 1/3; -1/4,1/2,1/4; 2,-2,1]$
    \end{enumerate}
\end{exercise}

\begin{exercise}
    Write the pseudocode and implement a program that computes the condition number of a matrix relative to the $\ell_1$ and $\ell_\infty$ norms.
\end{exercise}

\begin{exercise}
    Show by means of an example that the Frobenius norm for an  $n\times n$ matrix defined by 
    \begin{equation*}
        ||A||_F := \left(\sum_{i=1}^n\sum_{j=1}^n |a_{ij}|^2\right)^{1/2},
    \end{equation*}
    is not an \textbf{induced} matrix norm. Is it a matrix norm?
\end{exercise}

\begin{exercise}
    For each of the following linear systems, obtain a solution by graphical methods, if possible. Explain the results from a geometrical point of view.
    \begin{enumerate}
        \item 
        \begin{align*}
            x_1 + 2x_2 = 3,\\
            x_1 - x_2 =0.
        \end{align*}
        
        \item 
        \begin{align*}
            x_1 + 2x_2 = 3,\\
            2x_1 + 4x_2 =6.
        \end{align*}
        
        \item 
        \begin{align*}
            x_1 + 2x_2 = 0,\\
            2x_1 + 4x_2 =0.
        \end{align*}
        
        \item 
        \begin{align*}
            2x_1 + x_2 = -1,\\
            4x_1 - 2x_2 = -2\\
            x_1 - 3x_2 = 5.
        \end{align*}
        
         \item 
        \begin{align*}
            x_1 + 2x_2 = 0,\\
            2x_1 + (21/5)x_2 =0.
        \end{align*}
    \end{enumerate}
\end{exercise}

\begin{exercise}
    For the systems in the previous exercise compute the condition number of the associated matrix. Perturb the coefficients of the matrix and compute the new solution. If possible explain the condition number from a geometric point of view. If possible explain the propagation of the perturbations from a geometric point of view.
\end{exercise}

\begin{exercise}
    For the matrix $A = [0.001, 1; 1, 1]$ and the vector $b = [1, 2]'$, 
    \begin{enumerate}
        \item Compute the condition number of $A$ for the $\ell_2$ norm (use the command cond(A) in Matlab) and solve the system $Ax=b$
        \item Change the matrix $A$ and the data $b$ by two small random perturbations $\tilde{A}=A+\delta A$, $\tilde{b}=b+\delta b$, and solve the system $\tilde{A}\tilde{x}=\tilde{b}$.
        \item If $\tilde{x} = x +\delta x$, compare the relative errors 
        \begin{equation*}
            \frac{||\delta A||}{||A||},\ \frac{||\delta b||}{||b||} \text{ and } \frac{||\delta x||}{||x||}.
        \end{equation*}
    \end{enumerate}
\end{exercise}

\begin{exercise}
    For the matrix $A = [0.00000000000000000001, 1; 1, 1]$ and the vector $b = [1, 2]'$ repeat all the items in the previous exercise.
\end{exercise}

\begin{exercise}
    For the matrix $A = [0.00000000000000000001, 1; 1, 1]$ and the vector $b = [1, 2]'$ solve the system $Ax=b$ using LU decomposition with and without pivot. If the exact solution of the system is $x=[1,1]$. Call $x_1$ the solution obtained with pivot and $x_2$ the solution obtained without pivot. Compare the relative errors 
    \begin{equation*}
        \frac{||x_1-x||}{||x||} \text{ and } \frac{||x_2-x||}{||x||}.
    \end{equation*}
    Why is there such a difference? The condition number has anything to do with this difference?
\end{exercise}



\newpage 

\section{Iterative Techniques to Solve Linear Sytems}  



\newpage 
\subsection{One example of uses of linear algebra}

    Suppose that we want to find a solution to the following differential equation
    \begin{equation} \label{diff_eq}
        \begin{cases}
            y'' - p(x)y' - q(x) y = r(x), \text{ for } x\in (a,b)\\
            y(a) = \alpha, \ y(b) = \beta.
        \end{cases}
    \end{equation}
    Imposing the restriction $q(x)>Q_*>0$ in $[a,b]$. Assume that a unique solution of \eqref{diff_eq} exists with four continuous derivatives. Let us take $n+2$ equispaced nodes $a=x_0<x_1<\cdots<x_{n+1}=b$, and let $h=(b-a)/(n+1)$. On the interior points $x_1,\dots,x_n$ we have 
    \begin{align*}
        y'(x_i) &= \frac{1}{2h}[y(x_{i+1})-y(x_{i-1})] - \frac{h^2}{6}y'''(\eta_i)\\
        y''(x_i) &= \frac{1}{h^2}[y(x_{i+1})- 2 y(x_i)+y(x_{i-1})] -\frac{h^2}{12}y^{(4)}(\xi_i).
    \end{align*}
    Using the approximations to $y'$ and $y''$ (that is without considering the error) we can set what is called the difference equation 
    \begin{equation} \label{diff_eq}
        \begin{cases}
            \frac{1}{h^2}[u(x_{i+1})- 2 u(x_i)+u(x_{i-1})] - \frac{p(x_i)}{2h}[u(x_{i+1})-u(x_{i-1})] - q(x_i) u(x_i) = r(x_i), \text{ for } i= 1,\dots,n\\
            u(a) = \alpha, \ u(b) = \beta.
        \end{cases}
    \end{equation}
    Notice that we renamed the function as $u$ instead of $y$. That is because this two equations are not exactly the same, $u$ is a solution of the second system that does not consider the error term in the approximation. On the other hand, $y$ is the solution of the first system. It is expected that if $h$ is sufficiently small, then $u$ will be a good enough approximation of $y$ as we will see later. Now let us solve the last equation multiplying by $-h^2/2$ and rearranging it a little bit
    \begin{equation} \label{differences_eq}
        \begin{cases}
            -b_i u(x_{i-1}) + a_i u(x_i) - c_iu(x_{i+1}) = -h^2 r(x_i)/2, \text{ for } i= 1,\dots,n\\
            u(a) = \alpha, \ u(b) = \beta,
        \end{cases}
    \end{equation}
    where
    \begin{gather*}
        a_i = 1+\frac{h^2}{2}q(x_i), \qquad b_i = \frac{1}{2}\left[1+\frac{h}{2}p(x_i)\right]\\
        c_i = \frac{1}{2}\left[1-\frac{h}{2}p(x_i)\right].
    \end{gather*}
    Or in matrix form,
    \begin{equation*}
        \begin{bmatrix}
            1 & \\
            -b_1 &a_1 &-c_1 & & &\\
             &\ddots &\ddots &\ddots\\
             & & -b_n &a_n &-c_n\\
              & & & & 1
        \end{bmatrix}
        \begin{bmatrix}
            u(x_0)\\
            \vdots\\
            \vdots\\
            \vdots\\
            u(x_{n+1})
        \end{bmatrix} =
        \frac{-h^2}{2}
        \begin{bmatrix}
            \alpha\\
            r(x_1)\\
            \vdots\\
            r(x_n)\\
            \beta
        \end{bmatrix}
    \end{equation*}
    That is, a tridiagonal system. Now, if we take the step $h$ satisfying 
    \begin{equation}\label{h_cond}
        \frac{h}{2}|p(x_i)| \leq 1, \qquad i=1,\dots,n
    \end{equation}
    the matrix of the system becomes strictly diagonally dominant. Thus, we can find a solution $u(x_0),\dots,u(x_{n+1})$ using the methods described in this section. 
    
    It can be proven that the solution $u$ of the difference equation approximates the  function $y$ with order $\mathcal{O}(h^2)$ if we consider perfect arithmetic.
    \begin{theorem}
        If the step $h$ is taken satisfying \eqref{h_cond}, then 
        \begin{equation*}
            |u(x_i)-y(x_i)| \leq h^2 \left(\frac{M_4+2P^*M_3}{12Q_*}\right),
        \end{equation*}
        where $y(x)$ is the solution of the original differential equation, $u$ is the solution of the differences equation and 
        \begin{gather*}
            P^* = \max_{[a,b]}|p(x)|, \qquad M_3 = \max_{[a,b]}|y'''(x)|,\\
            M_4 =\max_{[a,b]}|y^{(4)}(x)|.
        \end{gather*}
    \end{theorem}
    
    \begin{proof}
        Using the midpoint formulas for the derivatives of $y$ and replacing in the original system \eqref{diff_eq} we obtain that $y$ satisfies
        \begin{multline}
            \frac{1}{h^2}[y(x_{i+1})- 2 y(x_i)+y(x_{i-1})] - \frac{p(x_i)}{2h}[y(x_{i+1})-y(x_{i-1})] - q(x_i) y(x_i) \\ -\frac{h^2}{12}y^{(4)}(\xi_i)  -\frac{h^2}{6}p(x_i)y^{(3)}(\eta_i) = r(x_i)
        \end{multline}
        Rearranging as with did with $u$ and multiplying on both sides by $-h^2/2$
        \begin{equation}\label{exact diff eq}
            -b_i y(x_{i-1}) + a_i y(x_i) - c_iy(x_{i+1}) - \frac{h^4}{12}\left(\frac{f^{(4)}(\xi)}{2}+p(x_i)f^{(3)}(\eta)\right)= -h^2 r(x_i)/2.
        \end{equation}
        Let us call $e_i := y(x_i)-u(x_i)$, then substracting \eqref{differences_eq} from \eqref{exact diff eq} we obtain,
        \begin{equation}
            -b_i e_{i-1} + a_i e_{i} -c_i e_{i+1}  - \frac{h^4}{12}\left(\frac{f^{(4)}(\xi)}{2}+p(x_i)f^{(3)}(\eta)\right) = 0.
        \end{equation}
        Rearranging,
        \begin{equation}
             a_i e_{i} = b_i e_{i-1} + c_i e_{i+1} + \frac{h^4}{12}\left(\frac{f^{(4)}(\xi_i)}{2}+p(x_i)f^{(3)}(\eta_)\right).
        \end{equation}
        Defining $e = \max |e_i|$, taking absolute value on both sides of the previous equation and using triangle inequality we get
        \begin{align*}
            |a_i||e_i| &\leq |b_i| e + |c_i| e + \frac{h^4}{12} \left(\frac{M_4}{2} + P^*M_3\right)\\
            &\leq (|b_i|+|c_i|)e + \frac{h^4}{12} \left(\frac{M_4}{2} + P^*M_3\right) \leq e + \frac{h^4}{12} \left(\frac{M_4}{2} + P^*M_3\right).
        \end{align*}
        Now, since 
        \begin{equation}
            1+\frac{h^2}{2}Q_* \leq 1 + \frac{h^2}{2}q(x_i) = a_i, 
        \end{equation}
        then
        \begin{equation}
            \left(1+\frac{h^2}{2}Q_*\right) e \leq e + \frac{h^4}{12} \left(\frac{M_4}{2} + P^*M_3\right).
        \end{equation}
        Solving for $e$
        \begin{equation}
            e \leq \frac{2}{h^2 Q_*}\frac{h^4}{12}\left(\frac{M_4}{2} + P^*M_3\right) = h^2 \left(\frac{M_4+2P^*M_3}{12Q_*}\right).
        \end{equation}
    \end{proof}
    
    With a very similar proof and assuming that the solution to the differential equation belongs to $C^6[a,b]$ it can be proven that the error in the approximation $(u_0,\dots,u_{n+1})$ with step $h$ is exactly
    \begin{equation}
        e_i = Ah^2  + \mathcal{O}(h^4),
    \end{equation}
    for some $A$ independent of $h$.\par
    With this in mind, one could use Richardson's extrapolation method to find an approximation with error of order $\mathcal{O}(h^4)$ as follows
    \begin{enumerate}
        \item Find a solution $u(x_i)$, $i=0,\dots,n+1$ of the differences equation with step $h$. The approximation will obey the equation $y(x_i) = u(x_i) + Ah^2+ \mathcal{O}(h^2)$.
        \item Find a solution $\tilde{u}(x_j)$, $j=0,\dots,2n+3$ of the differences equation with step $h/2$.  The approximation will obey the equation $y(x_i) = \tilde{u}(x_i) + A\frac{h^2}{4}+ \mathcal{O}(\frac{h^2}{4})$.
        \item For the points of the form $x_0+ih$, with $h=(b-a)/(n+1)$, use the estimations $u(x_i)$ and $\tilde{u}(x_i)$ to compute a third estimation $\tilde{\tilde{u}}(x_i)$ of order $\mathcal{O}(h^4)$.
    \end{enumerate}
  
  
    There is still something else that can be analyzed in this example and that is the error in the difference equations solution due to round-off. In practice when we apply a numerical method to solve \eqref{differences_eq} we obtain not $u$ but an approximation $v$ satisfying
    
    \begin{equation} \label{differences_eq}
        \begin{cases}
            -b_i v(x_{i-1}) + a_i v(x_i) - c_iv(x_{i+1}) = -h^2 r(x_i)/2 + res(x_i), \text{ for } i= 1,\dots,n\\
            v(a) = \alpha + res(x_0) , \ v(b) = \beta + res(x_{n+1}).
        \end{cases}
    \end{equation}
    
    The notation $res$ for the round-off error of $u$ comes from the name residual. It can be computed as 
    
    \begin{equation*}
        \begin{bmatrix}
            1 & \\
            -b_1 &a_1 &-c_1 & & &\\
             &\ddots &\ddots &\ddots\\
             & & -b_n &a_n &-c_n\\
              & & & & 1
        \end{bmatrix}
        \begin{bmatrix}
            u(x_0)\\
            \vdots\\
            \vdots\\
            \vdots\\
            u(x_{n+1})
        \end{bmatrix} -
        \frac{-h^2}{2}
        \begin{bmatrix}
            \alpha\\
            r(x_1)\\
            \vdots\\
            r(x_n)\\
            \beta
        \end{bmatrix} =
        \begin{bmatrix}
            res(x_0)\\
            res(x_1)\\
            \vdots\\
            res(x_n)\\
            res(x_{n+1})
        \end{bmatrix}
    \end{equation*}
    The question is how can we use this residual to estimate the error of using $v$ instead of $u$. That is, estimating
    \begin{equation}
        E = \max|u_i - v_i|, \qquad i=0,\dots,n+1. 
    \end{equation}
    
    \begin{theorem}
        Let $u$ be the exact solution of the differences equation \eqref{differences_eq}, and let $v$ be the solution obtained by any numerical method with residual error given by $res := \max{res(x_i)}$. Assume that $q(x) \geq Q_* > 0$ in the original differential equation. Then,
        \begin{equation}
            E = \max|u(x_i) - v(x_i)| \leq \frac{2 res}{h^2Q_*}.
        \end{equation}
    \end{theorem}
    
    \begin{proof}
        The proof is analogous to the previous theorem. 
    \end{proof}
    
    \begin{corollary} 
        The error in the solution of the original differential equation $y$ when approximated by the vector $v$ at the mesh points $x_0,\dots,x_{n+1}$ obtained by numerically solving the differences equation can be bounded by 
        \begin{equation}
            E = \max|y(x_i)-v(x_i)| \leq h^2 \left(\frac{M_4+2P^*M_3}{12Q_*}\right) + \frac{2 res}{h^2Q_*}.
        \end{equation}
    \end{corollary}
    
    
  

  
\newpage 



\newpage 
\section{Another example of uses of linear algebra}
    \begin{problem}
        Let us go back to the problem of finding zeros of functions. But this time we will do it for functions of several variables of the form $F:\R^n\rightarrow\R^n$. The method we will see is just Newton's for several variables. 
    \end{problem}
    
    Let us denote $F(x) = (f_1(x),\dots,f_n(x))$. Assume that $F$ is differentiable and has continuous second partial derivatives. If we know the value of $F$ at a point $x^0$, we can expand $F$ at close points using the Jacobian or derivative of $F$ 
    \begin{equation}\label{taylor eq for Rn}
        F(x) = F(x^0) + JF(x^0)(x-x^0) + \mathcal{O}(||x-x^0||^2),
    \end{equation}
    where 
    \begin{equation}
        JF(x^0) =
        \begin{bmatrix}
            \frac{\partial f_1}{\partial x_1}(x^0) &\cdots & \frac{\partial f_1}{\partial x_n}(x^0)\\
            \vdots & &\vdots\\
            \frac{\partial f_n}{\partial x_1}(x^0) &\cdots & \frac{\partial f_n}{\partial x_n}(x^0)\\
        \end{bmatrix}.
    \end{equation}
    
    If we drop off the higher order term in \eqref{taylor eq for Rn}, we obtain
    
    \begin{equation}
        F(x)\approx F(x^0) + JF(x^0)(x-x^0).
    \end{equation}
    
    To find a zero then a one idea would be to set this equation to zero and solve for $x$,
    
    \begin{gather}
        0 = F(x^0) + JF(x^0)(x-x^0),\\
        JF(x^0)(x-x^0) = -F(x^0),\\ 
        x = -JF^{-1}(x^0)F(x^0) + x^0. \label{newton's it multidim}
    \end{gather}
    
    Of course, this $x$ won't be an actual zero but an approximation so what we can do is create an iterative technique instead. Set $x^nn$ such that
    
    \begin{equation}
        x^k = x^{k-1} + h,
    \end{equation}
    
    with $h$ satisfying
    
    \begin{equation}
        JF(x^{k-1})h = -F(x^{k-1}).   
    \end{equation}
    
    Notice that this is nothing more than a fixed point iteration method. In fact, if $x$ is a fixed point of the last equation, it will be a zero of $F$. CHECK THIS!\par
    
    
    \newpage
    \begin{problem}
        How can we guarantee convergence of this fixed point iteration?
    \end{problem}
    
    Let us work in general for a fixed point iteration scheme of the form
    \begin{equation}
        x^k = g(x^{k-1}) = (g_1(x^{k-1}),\dots,g_n(x^{k-1})),
    \end{equation}
    and suppose that $p$ is a fixed point of $g$. That is $g(p)=p$. Now, we do the usual trick to check convergence to each coordinate function,
    \begin{align*}
        x^k_i-p_i &= g_i(x^{k-1})-g_i(p) \\
        &= \sum_{j=1}^n \frac{\partial g_i}{\partial x_j}(\xi_k) (x^{k-1}_i-p_i),
    \end{align*}
    where $\xi_i$ is a point in the segment of line with endpoints $x^{k-1}$ and $p$.\\
    Taking infinite norm on the previous equation,
    \begin{align*}
        \max_i\{|x^k_i-p_i|\} &= \max_i \left\{ \left|\sum_{j=1}^n \frac{\partial g_i}{\partial x_j}(\xi_n) (x^{k-1}_i-p_i)\right|\right\}\\
        &\leq  \max_i\left\{\sum_{j=1}^n\left|\frac{\partial g_i}{\partial x_j}(\xi_n)\right|\right\} ||x^s - p||_\infty.
    \end{align*}
    Therefore, if we ask
    \begin{equation}
        ||JG(x)||_\intfty =\max_i\left\{\sum_{j=1}^n\left|\frac{\partial g_i}{\partial x_j}(\xi_n)\right|\right\} < k <1,
    \end{equation}
    we can get convergence of the fixed point iteration as we did for the one dimensional case.
    
    Taking into account that for any matrix near the zero matrix, its norm is going to be less than one, we can get the following theorem as we did for the one dimensional case long time ago,
    
    \begin{theorem}
        Let $p$ be a solution of $g(x) =x$. Suppose a number $\delta>0$ exists with the properties
        \begin{enumerate}
            \item $\partial g_i/ \partial x_j$ is continuous on $N_\delta:= \{x/ ||x-p||<\delta\}$, for all $i,j$.
            \item $\partial^2 g_i/ (\partial x_j\partial x_k)$ is continuous, and $|\partial^2 g_i/ (\partial x_j\partial x_k)|< M$ when $x\in N_\delta$, for all $i,j,k$.
            \item $\partial g_i(p)/\partial x_j = 0$ for all $i,j$.
        \end{enumerate}
        Then a number $\tilde{\delta}\leq \delta$ exists such that the sequence generated by $x^k + g(x^{k-1})$ converges quadratically to $p$ for any choice $x^0$, provided that $||x^0-p||<\tilde{\delta}$. Moreover,
        \begin{equation}
            ||x^k - p||_\infty \leq \frac{n^2M}{2}||x^{k-1}-p||_\infty.
        \end{equation}
    \end{theorem}
    
    \newpage
    \begin{problem}
        How can we use this theorem to prove convergence of Newton's method?
    \end{problem}
    
    Notice from \eqref{newton's it multidim} that Newton's iteration comes from an equation of the form
    \begin{equation}
        x = g(x) := x - \phi(x) F(x),
    \end{equation}
    where $\phi$ is a predefined family of matrices. Assume for the moment that we do not know $\phi$ and want to pick it satisfying the hypothesis of the convergence theorem. Naming $b_{ij}$ each entry of the matrix $\phi$ we obtain,
    
    \begin{equation}
        g_i(x) = x_i - \sum_{j=1}^n b_{ij}(x)f_j(x). 
    \end{equation}
    So,
    \begin{equation}
        \frac{\partial g_i}{\partial x_k}(x) = 
        \begin{cases}
            1- \sum_{j=1}^n \left(b_{ij}(x)\frac{\partial    f_j}{\partial x_k}(x) + \frac{\partial b_{ij}}{\partial x_k}(x)f_j(x)\right), \text{ if } i=k,\\
            -\sum_{j=1}^n \left(b_{ij}(x)\frac{\partial    f_j}{\partial x_k}(x) + \frac{\partial b_{ij}}{\partial x_k}(x)f_j(x)\right), \text{ if } i\neq k.
        \end{cases}
    \end{equation}
    Setting up $ \frac{\partial g_i}{\partial x_k}(p) = 0$ for all $i$ and remembering that $F(p) = 0$, we get 
    \begin{align}
        \sum_{j=1}^n b_{ij}(p)\frac{\partial    f_j}{\partial x_k}(p) = 1, \text{ for } i=k \\
        \sum_{j=1}^n b_{ij}(p)\frac{\partial    f_j}{\partial x_k}(p) = 0, \text{ for } i\neq k. 
    \end{align}
    That is, $\phi(p)JF(p) = I$. Thus, to satisfy the hypothesis of the convergence theorem we could choose 
    \begin{equation}
        \phi(x) = JF^{-1}(x),
    \end{equation}
    which gives us exactly Newton's method!
    
    
    \bigskip
    \bigskip
    \hrule
    \begin{exercise}
        Write the pseudo code for Newton's method in several variables and implement it in MatLab.
    \end{exercise}
    